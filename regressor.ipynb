{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6.491858,
      "end_time": "2021-02-25T10:29:21.857292",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-02-25T10:29:15.365434",
      "version": "2.2.2"
    },
    "colab": {
      "name": "regressor.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bob-Gohardani/nlp-ml/blob/main/regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.008751,
          "end_time": "2021-02-25T10:29:20.987527",
          "exception": false,
          "start_time": "2021-02-25T10:29:20.978776",
          "status": "completed"
        },
        "tags": [],
        "id": "-3JPynKXY1TO"
      },
      "source": [
        "# Regressor\n",
        "\n",
        "A multivariate polynomial regressor with scaling,validating,training and testing written in pure python without any sort of external libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.006514Z",
          "iopub.status.busy": "2021-02-25T10:29:21.005750Z",
          "iopub.status.idle": "2021-02-25T10:29:21.010301Z",
          "shell.execute_reply": "2021-02-25T10:29:21.010871Z"
        },
        "papermill": {
          "duration": 0.015222,
          "end_time": "2021-02-25T10:29:21.011184",
          "exception": false,
          "start_time": "2021-02-25T10:29:20.995962",
          "status": "completed"
        },
        "tags": [],
        "id": "6_2h7xLAY1TQ"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import random\n",
        "from itertools import combinations_with_replacement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.029455Z",
          "iopub.status.busy": "2021-02-25T10:29:21.028784Z",
          "iopub.status.idle": "2021-02-25T10:29:21.033959Z",
          "shell.execute_reply": "2021-02-25T10:29:21.033441Z"
        },
        "papermill": {
          "duration": 0.015343,
          "end_time": "2021-02-25T10:29:21.034113",
          "exception": false,
          "start_time": "2021-02-25T10:29:21.018770",
          "status": "completed"
        },
        "tags": [],
        "id": "Bp02NZIpY1TR"
      },
      "source": [
        "# python regressor.py -t set.txt < in.txt > out.txt\n",
        "train_data = []\n",
        "scaled_data = {}\n",
        "validation_sets = []\n",
        "training_sets = []\n",
        "dim = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.061088Z",
          "iopub.status.busy": "2021-02-25T10:29:21.060367Z",
          "iopub.status.idle": "2021-02-25T10:29:21.063099Z",
          "shell.execute_reply": "2021-02-25T10:29:21.063595Z"
        },
        "papermill": {
          "duration": 0.022253,
          "end_time": "2021-02-25T10:29:21.063790",
          "exception": false,
          "start_time": "2021-02-25T10:29:21.041537",
          "status": "completed"
        },
        "tags": [],
        "id": "IoL9XnHxY1TS"
      },
      "source": [
        "# find min / max of the dataset\n",
        "def findMinMax():\n",
        "    ran = 0\n",
        "    dummy = 0\n",
        "\t# open the file, read each line of it, add it to a list object \"l\" then add \"l\" to train_data list\n",
        "    with open(sys.argv[2], 'r') as f:\n",
        "        for line in f:\n",
        "            if line != '\\n' and line != '\\n\\r':\n",
        "                l = [float(x) for x in line.replace('\\n', '').replace('\\r', '').strip().split(' ')]\n",
        "\t\t\t\t# length of each line-1 is dimension\n",
        "                global dim\n",
        "                dim = len(l) - 1 \n",
        "                train_data.append(l)\n",
        "\t\t\t\t\n",
        "                if dummy == 0:\n",
        "                    ran = len(l) \n",
        "                    dummy += 1\n",
        "\t# create empty dictionary and give it free space\n",
        "    dct = {}\n",
        "    for i in range(ran):\n",
        "        dct[i] = []\n",
        "\n",
        "\t# we open the file again and this time attach the data to the dictionary that we created earlier\n",
        "    with open(sys.argv[2], 'r') as f:\n",
        "        for line in f:\n",
        "            if line != '\\n' and line != '\\n\\r':\n",
        "                l = [float(x) for x in line.replace('\\n', '').replace('\\r', '').strip().split(' ')]\n",
        "                for i in range(ran):\n",
        "                    dct[i].append(l[i])\n",
        "# in this part we try to find min and max for each iteration in the dictioanry, empty the dict and add min/max to it\n",
        "    for key in dct:\n",
        "        high = max(dct[key])\n",
        "        low = min(dct[key])\n",
        "        dct[key] = []\n",
        "        dct[key].append(low)\n",
        "        dct[key].append(high)\n",
        "    return dct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.086955Z",
          "iopub.status.busy": "2021-02-25T10:29:21.086184Z",
          "iopub.status.idle": "2021-02-25T10:29:21.089226Z",
          "shell.execute_reply": "2021-02-25T10:29:21.089806Z"
        },
        "papermill": {
          "duration": 0.01853,
          "end_time": "2021-02-25T10:29:21.090005",
          "exception": false,
          "start_time": "2021-02-25T10:29:21.071475",
          "status": "completed"
        },
        "tags": [],
        "id": "Mm16fGnBY1TS"
      },
      "source": [
        "# in this function we scale the data from 0 to 1\n",
        "def ScaleData():\n",
        "    dct = findMinMax() \n",
        "\t# here we loop through the train_data list and scale each row based on the min/max values that we have from dictionary\n",
        "    for j in range(len(train_data)):\n",
        "        scaled_data[j] = []\n",
        "        l = train_data[j]\n",
        "        for i in range(len(l)):\n",
        "            if i != len(l)-1:\n",
        "                y = (2 * ((l[i] - dct[i][0]) / (dct[i][1] - dct[i][0]))) - 1\n",
        "                scaled_data[j].append(y)\n",
        "\t\t\t# last element of each row is the Y prediction and we dont need to scale it\n",
        "            else:\n",
        "                scaled_data[j].append(l[i])\n",
        "\n",
        "\n",
        "# not used!!!\n",
        "#def chunker_list(list, split):\n",
        "   # L = list[:]\n",
        "   # random.shuffle(L)\n",
        "   # return [L[x::split] for x in range(split)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.117084Z",
          "iopub.status.busy": "2021-02-25T10:29:21.116345Z",
          "iopub.status.idle": "2021-02-25T10:29:21.119875Z",
          "shell.execute_reply": "2021-02-25T10:29:21.119224Z"
        },
        "papermill": {
          "duration": 0.022119,
          "end_time": "2021-02-25T10:29:21.120018",
          "exception": false,
          "start_time": "2021-02-25T10:29:21.097899",
          "status": "completed"
        },
        "tags": [],
        "id": "9m4wDfEHY1TT"
      },
      "source": [
        "# in this function we split data for the validation / train sets\t\n",
        "def splitData():\n",
        "    ScaleData()\n",
        "    number_of_split= 2\n",
        "    setj = []\n",
        "\t\n",
        "\t# save all scaled data from dictionary into a list :\n",
        "    for key, value in scaled_data.iteritems():\n",
        "        temp = value\n",
        "        setj.append(temp)\n",
        "\n",
        "\t# since split is two, we have four subsets and here we find length of each of them\n",
        "    set_length = len(setj)\n",
        "    totalsubset= int(number_of_split)*2\n",
        "    len_of_each= set_length/totalsubset\n",
        "\n",
        "\n",
        "\t# here we  create the train set based on the repeated random sub_sampling algorithm:\n",
        "    for i in range(0, number_of_split):\n",
        "        sub_set = []\n",
        "\t\t# loop through for length of each subset\n",
        "        for j in range(0,int(len_of_each)):\n",
        "           \n",
        "            sub_line = []\n",
        "\t\t\t# find a random number from set and find element related to it\n",
        "            x=random.randint(0,len(setj)-1)\n",
        "            for a in range (len(setj[0])):\n",
        "                sub_line.append(setj[x][a])\n",
        "\t\t\t# remove that element\n",
        "            setj.remove(setj[x][:])\n",
        "            sub_set.append(sub_line)\n",
        "        training_sets.append(sub_set)\n",
        "\n",
        "# same as above but fpr validation subsets:\n",
        "    for i in range(0, number_of_split):\n",
        "        sub_set = []\n",
        "        for j in range(0, int(len_of_each)):\n",
        "            sub_line = []\n",
        "            x = random.randint(0, len(setj) - 1)\n",
        "            for a in range (len(setj[0])):\n",
        "                sub_line.append(setj[x][a])\n",
        "            setj.remove(setj[x][:])\n",
        "            sub_set.append(sub_line)\n",
        "        validation_sets.append(sub_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.165963Z",
          "iopub.status.busy": "2021-02-25T10:29:21.164676Z",
          "iopub.status.idle": "2021-02-25T10:29:21.168446Z",
          "shell.execute_reply": "2021-02-25T10:29:21.167884Z"
        },
        "papermill": {
          "duration": 0.040608,
          "end_time": "2021-02-25T10:29:21.168585",
          "exception": false,
          "start_time": "2021-02-25T10:29:21.127977",
          "status": "completed"
        },
        "tags": [],
        "id": "GmSf-51BY1TT"
      },
      "source": [
        "def train(n,k,input_data):\n",
        "    gamma = 0.09\n",
        "# set max iterations of gradient descent to 5000 iterations and set threshold to 0.0001\n",
        "    max_iters = 5000\n",
        "    threshold = 0.0001   \n",
        "    params = [] \n",
        "    indiceZ = []\n",
        "    Y = []\n",
        "    N = 0\n",
        "    gradQ = 0\n",
        "    inputz = []\n",
        "\n",
        "    k = int(k)\n",
        "    n = int(n)\n",
        "    desc = []\n",
        "# empty string\n",
        "    nstring = \"\"\n",
        "# create a string of numbers from 0 to n:\n",
        "    for i in range(n+1):\n",
        "        nstring+= str(i)\n",
        "#based on the k create all combinations of nstring that is possible and unique then turn it from string to number list:\n",
        "    l =  list(combinations_with_replacement(nstring, k))\n",
        "    l = [x for x in l[::-1]]\n",
        "\n",
        "# create empty list then  join all inner elements of l list to the l2 list except last element from biggest to smallest\n",
        "    l2 = []\n",
        "    for i in l:\n",
        "        l1 = list(i)\n",
        "        l2.append(\"\".join([x for x in l1[::-1]]))\n",
        "    l2 = sorted(l2, reverse = True)\n",
        "\n",
        "# transform l2 into the description of the linear regression:\n",
        "    for i in l2:\n",
        "        string = \"\"\n",
        "        string += \" \".join(i)\n",
        "        string += \" \"+str(round(random.uniform(-1, 1), 1))\n",
        "        desc.append([float(i) for i in string.split()])\n",
        "\n",
        "# from description divide parameters and indices:\n",
        "    for line in desc:\n",
        "        params.append(line[len(line) - 1])  \n",
        "        indiceZ.append(line[:-1])\n",
        "\n",
        "# seperate input data and result Y\n",
        "    input_arr = [line[:-1] for line in input_data]\n",
        "    Y = [line[-1] for line in input_data]\n",
        "\n",
        "    # here first we loop through each row of inputs then we go through indices, here our goal is if we have x1*x2*x3 or x2^2 get the full number and save it to a list:\n",
        "    # this basically means we have one variable for each param (x1,x2,...) so it turns into a linear regression from polynomial regression (lowers degree to 1)\n",
        "    for it, input in enumerate(input_arr):\n",
        "        x = []\n",
        "        for indices in indiceZ[:-1]:\n",
        "            value = 1.0\n",
        "            for i in indices:\n",
        "                if i != 0:\n",
        "                    value *= input[int(i)-1]\n",
        "            x.append(value)\n",
        "        inputz.append(x)\n",
        "\n",
        "\n",
        "# here we start creating the description again, but this time base it on a k=1 linear regression\n",
        "    n = len(params)-1\n",
        "    k =1\n",
        "    desc = []\n",
        "    nstring = \"\"\n",
        "    for i in range(n+1):\n",
        "        nstring+= str(i)\n",
        "    l =  list(combinations_with_replacement(nstring, k))\n",
        "    l = [x for x in l[::-1]]\n",
        "    l2 = []\n",
        "    for i in l:\n",
        "        l1 = list(i)\n",
        "        l2.append(\"\".join([x for x in l1[::-1]]))\n",
        "    l2 = sorted(l2, reverse = True)\n",
        "\n",
        "    for i in l2:\n",
        "        string = \"\"\n",
        "        string += \" \".join(i)\n",
        "        string += \" \"+str(round(random.uniform(-1, 1), 1))\n",
        "        desc.append([float(i) for i in string.split()])\n",
        "\n",
        "    params = []\n",
        "    for line in desc:\n",
        "        params.append(line[len(line) - 1]) \n",
        "        indiceZ.append(line[:-1])\n",
        "\n",
        "    N = len(inputz)\n",
        "\n",
        "# we calculate the function f(x,p) \n",
        "# multiplie each parameter by its variable then add them up together\n",
        "    def calc_f(i):\n",
        "        line = inputz[i]\n",
        "        total = 0\n",
        "        start = 0\n",
        "        for l in desc:\n",
        "            s = float(params[start])\n",
        "            start += 1\n",
        "            if(l[0]) != 0 :\n",
        "                s *= line[int(l[0]) -1]\n",
        "            total += s\n",
        "        return total\n",
        "\n",
        "# this function helps to select which x for derivative \n",
        "# basically (0, N) is the ith parameter and here we find Xj that matches with that\n",
        "# we give it id of parameter and it returns it back\n",
        "    def find_num(param):\n",
        "        return desc[param][0]-1\n",
        "\n",
        "# here we calculate the derivate based on the formula for each parameter, last parameter doesnt have \"x\" multiplication\n",
        "    def derivative(num):\n",
        "        sigma = 0\n",
        "# n is how many input variables/parameters we have\n",
        "        for i in range(0, N):\n",
        "            if num == len(params)-1:\n",
        "                sigma += (calc_f(i) - Y[i])\n",
        "            else:\t\t\t\t\t\t\t  # returns variable in the row connected to the param\n",
        "                sigma += (calc_f(i) - Y[i]) * inputz[i][int(find_num(num))]\n",
        "        if(N != 0):\n",
        "            return (1 / float(N)) * sigma\n",
        "\n",
        "\n",
        "# in this function we calculate gradient descent for each of the parameters, update the parameter and calculate the quality parameter\n",
        "    def gradient_descent():\n",
        "        global gradQ\n",
        "        x = 0\n",
        "        for i in range(0, len(params)):\n",
        "            d = derivative(i)\n",
        "            params[i] = params[i] - gamma * d\n",
        "            x += d**2\n",
        "        x = math.sqrt(x)    \n",
        "        gradQ = x\n",
        "        return(gradQ)\n",
        "\n",
        "# in this loop we calculate gradient until either we reach max iteration or we have quality smaller than the threshold:\n",
        "    for u in range(0, max_iters):\n",
        "        gradQ = gradient_descent()\n",
        "        if (gradQ < threshold):\n",
        "            break\n",
        "    results = []\n",
        "    results.append([n, 1])\n",
        "\n",
        "# at end we add all new parameters as a string and send it for test function:\n",
        "    for i in range(0, len(desc)):\n",
        "        string = \"\"\n",
        "        for j in range(0, len(desc[i])-1):\n",
        "            string += str(desc[i][j])+\" \"\n",
        "        string += str(params[i])\n",
        "        x = string.split(\" \")\n",
        "        results.append(x)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.205883Z",
          "iopub.status.busy": "2021-02-25T10:29:21.205136Z",
          "iopub.status.idle": "2021-02-25T10:29:21.208629Z",
          "shell.execute_reply": "2021-02-25T10:29:21.208089Z"
        },
        "papermill": {
          "duration": 0.032323,
          "end_time": "2021-02-25T10:29:21.208802",
          "exception": false,
          "start_time": "2021-02-25T10:29:21.176479",
          "status": "completed"
        },
        "tags": [],
        "id": "aw-SAv6dY1TX"
      },
      "source": [
        "# this is the method to test our data :\n",
        "def testIt(n1,k1, description, a, data=None):\n",
        "# get n/k/description from program:\n",
        "    n = description[0][0]\n",
        "    k = description[0][1]\n",
        "    desc = description[1:] # first line is n and k\n",
        "\n",
        "# if num is zero means just ignore that variable otherwise return variable related to the number from description\n",
        "    def power(num, line):\n",
        "        if num == 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return line[int(num)-1]\n",
        "\t\n",
        "\t# if it is a validation test there is no need for scaling otherwise we scale the test data:\n",
        "    scaled_data_test = []\n",
        "    if data:\n",
        "        scaled_data_test = [line[:-1] for line in data]\n",
        "\n",
        "    else:\n",
        "        # scale the data:\n",
        "        dct = findMinMax()\n",
        "        test_data = []\n",
        "\n",
        "        for line in sys.stdin:\n",
        "            if line != '\\n' and line != '\\n\\r':\n",
        "                l = [float(x) for x in line.replace('\\n', '').replace('\\r', '').strip().split(' ')]\n",
        "                test_data.append(l)\n",
        "\n",
        "        for i in range(len(test_data)):\n",
        "            scaled_data_test.append([])\n",
        "\n",
        "        for j in range(len(test_data)): \n",
        "            l_testData = test_data[j]\n",
        "            for i in range(len(l_testData)): \n",
        "                y = (2 * ((l_testData[i] - dct[i][0]) / (dct[i][1] - dct[i][0]))) - 1\n",
        "                scaled_data_test[j].append(y)\n",
        "\t\t\t\t\n",
        "# here we create a new description file so we can transform our scaled data from polynomial to linear (description from train function is linear)\n",
        "    indiceZ = []\n",
        "    k1 = int(k1)\n",
        "    n1 = int(n1)\n",
        "    desc1 = []\n",
        "    nstring = \"\"\n",
        "    for i in range(n1+1):\n",
        "        nstring+= str(i)\n",
        "    l =  list(combinations_with_replacement(nstring, k1))\n",
        "    l = [x for x in l[::-1]]\n",
        "    l2 = []\n",
        "    for i in l:\n",
        "        l1 = list(i)\n",
        "        l2.append(\"\".join([x for x in l1[::-1]]))\n",
        "    l2 = sorted(l2, reverse = True)\n",
        "\n",
        "    for i in l2:\n",
        "        string = \"\"\n",
        "        string += \" \".join(i)\n",
        "        string += \" \"+str(round(random.uniform(-1, 1), 1))\n",
        "        desc1.append([float(i) for i in string.split()])\n",
        "\n",
        "    for line in desc1:\n",
        "        indiceZ.append(line[:-1])\n",
        "\n",
        "    input_arr = scaled_data_test\n",
        "    inputz = []\n",
        "    for it, input in enumerate(input_arr):\n",
        "        x = []\n",
        "        for indices in indiceZ[:-1]:\n",
        "            value = 1.0\n",
        "            for i in indices:\n",
        "                if i != 0:\n",
        "                    value *= input[int(i)-1]\n",
        "            x.append(value)\n",
        "        inputz.append(x)\n",
        "    scaled_data_test = inputz\n",
        "\n",
        "\t# here we multiplie the parameters by the variables and return the results:\n",
        "    results = []\n",
        "    for line in scaled_data_test:\n",
        "        total = 0\n",
        "        for m in desc:\n",
        "            s = float(m[k]) # parameter is last element in each description row\n",
        "            for i in range(0, k):\n",
        "                s *= power(float(m[i]), line)\n",
        "            total += s\n",
        "        results.append(total)\n",
        "    if a == \"final\":\n",
        "        for res in results:\n",
        "            print(res)\n",
        "\n",
        "    elif a == \"valid\":\n",
        "        return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:29:21.235846Z",
          "iopub.status.busy": "2021-02-25T10:29:21.235140Z",
          "iopub.status.idle": "2021-02-25T10:29:21.238489Z",
          "shell.execute_reply": "2021-02-25T10:29:21.237967Z"
        },
        "papermill": {
          "duration": 0.021648,
          "end_time": "2021-02-25T10:29:21.238626",
          "exception": false,
          "start_time": "2021-02-25T10:29:21.216978",
          "status": "completed"
        },
        "tags": [],
        "id": "wuQn5u1dY1Ta"
      },
      "source": [
        "def final_predict(): \n",
        "    # # call split function and make an evaluations list\n",
        "    # splitData()\n",
        "    # evaluations = []\n",
        "    # loop through k from 1 to 6:\n",
        "    for k in range(1, 6):\n",
        "        qualityList = []\n",
        "        qualityList.append(k)\n",
        "\n",
        "    #loop through length of our training/validation subsets:\n",
        "        for i in range(len(training_sets)):\n",
        "            train_set = training_sets[i]\n",
        "    # create description from the training function :\n",
        "            desc_valid = train(dim, k, train_set)\n",
        "            validation_set = validation_sets[i]\n",
        "    # test out validation data based on the description we got and the k we give it :\n",
        "            out_arr = testIt(dim, k, desc_valid, \"valid\", validation_set)\n",
        "\n",
        "    # get the results from the validation test :\n",
        "            lines_1 = []\n",
        "            lines_1 = [line[-1] for line in validation_set]  \n",
        "            total = 0\n",
        "    # loop through all results and compare them with real results then add the quality based on that to evaluation set:\n",
        "            for i in range(len(out_arr)):\n",
        "                total += pow((out_arr[i] - lines_1[i]), 2)\n",
        "            Quality =  total / (2 * len(out_arr))\n",
        "            qualityList.append(Quality)\n",
        "        evaluations.append(qualityList)\n",
        "    # first element is K :\n",
        "    eva_1 =[line[1:] for line in evaluations]\n",
        "    # get averge of each subset of evaluation :\n",
        "    eva_1 =[sum(line)/len(line) for line in eva_1]\n",
        "    eva_2 =[line[0] for line in evaluations]\n",
        "\n",
        "    # find the k with lowest value of error:\n",
        "    hyper = eva_2[eva_1.index(min(eva_1))]\n",
        "\n",
        "    #get final description from train program based on final K then test it to get results:\n",
        "    description_final = train(dim, hyper, scaled_full) \n",
        "    testIt(dim, hyper, description_final, \"final\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}