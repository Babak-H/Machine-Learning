{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-means.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvpnOk3uSBldnoN+WNJzhx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bob-Gohardani/nlp-ml/blob/main/k_means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNIZAJjoUyWk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNgveujeVM_6"
      },
      "source": [
        "# dataset has two features: how many packets sent per second / size of the packet\n",
        "def load_dataset(name):\n",
        "  return np.loadtxt(name)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJp9FLycVdQy"
      },
      "source": [
        "# euclidian distance between two points\n",
        "def euclidian(a, b):\n",
        "  return np.linalg.norm(a-b)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p06TpuUPVoDN"
      },
      "source": [
        "# epsilon : max error that we can accept\n",
        "def  kmeans(k, epsilon=0.01, distance=\"euclidian\"):\n",
        "    if distance == \"euclidian\":\n",
        "        dist_method = euclidian\n",
        "        \n",
        "    # load dataset\n",
        "    dataset = load_dataset('durudataset.txt')\n",
        "    # get number of rows and columns(features) from dataset\n",
        "    num_instances, num_features = dataset.shape\n",
        "    # these are the initial centroids of the clusters k (each of them is an element that exists in our dataset)\n",
        "    prototypes = dataset[np.random.randint(0, num_instances-1, size = k)]\n",
        "    # save centroids of each iteration\n",
        "    prototypes_old = np.zeros(prototypes.shape)\n",
        "    # store cluster and which cluster each item belongs tp\n",
        "    belongs_to = np.zeros((num_instances, 1))\n",
        "    # get euclidian distance between current centroids and older version (if it doesnt change then we are at last iteration of k-means algorithm)\n",
        "    norm = dist_method(prototypes, prototypes_old)\n",
        "    iteration = 0\n",
        "\n",
        "    while norm > epsilon: # until distance between 2 generation of centroids is bigger than margin error\n",
        "        iteration += 1\n",
        "        # get euclidian distance between current centroids and older version\n",
        "        norm = dist_method(prototypes, prototypes_old)\n",
        "        # loop through each element of the dataset\n",
        "        for index_instance, instance in enumerate(dataset):\n",
        "            # create a distance vector with K rows (columns are dstance between centroid and each element)\n",
        "            dist_vec = np.zeros((k,1))\n",
        "            # loop through all centroids and calculate distance between them and the element\n",
        "            for index_prototype, prototype in enumerate(prototypes):\n",
        "                dist_vec[index_prototype] = dist_method(prototype, instance)\n",
        "            # then save which cluster the element belongs to\n",
        "            belongs_to[index_instance, 0] = np.argmin(dist_vec)\n",
        "        # this is just a buffer matrix to save the data before setting it to final centroids\n",
        "        tmp_prototypes = np.zeros((k, num_features))\n",
        "\n",
        "        # loop through all centroids\n",
        "        for index in range(len(prototypes)):\n",
        "            # find the index of all elements that belong to this cluster\n",
        "            belong_to_this = [i for i in range(len(belongs_to)) if belongs_to[i] == index]\n",
        "            # get the averge position for all elements of this cluster and set it as new centroid\n",
        "            prototype = np.mean(dataset[belong_to_this], axis=0)\n",
        "            tmp_prototypes[index, :] = prototype\n",
        "\n",
        "        prototypes = tmp_prototypes\n",
        "\n",
        "    # at the end return final centroid and belongs_to matrices\n",
        "    return prototypes, belongs_to"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDXmnoUlXQY4"
      },
      "source": [
        "def execute():\n",
        "    #train our model on the data and find the results\n",
        "    centroids, belongs_to = kmeans(2)\n",
        "    print(centroids)\n",
        "    print(\"-----------------\")\n",
        "    print(belongs_to)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dylvnnQhXaYx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}