{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 7.362053,
      "end_time": "2021-02-25T10:51:08.087391",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-02-25T10:51:00.725338",
      "version": "2.2.2"
    },
    "colab": {
      "name": "recommendation-system.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bob-Gohardani/nlp-ml/blob/main/recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.007962,
          "end_time": "2021-02-25T10:51:06.327811",
          "exception": false,
          "start_time": "2021-02-25T10:51:06.319849",
          "status": "completed"
        },
        "tags": [],
        "id": "RKVsg6iCZP5S"
      },
      "source": [
        "Reccomendation engine written in pure Python, only external library is numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-02-25T10:51:06.345097Z",
          "iopub.status.busy": "2021-02-25T10:51:06.344470Z",
          "iopub.status.idle": "2021-02-25T10:51:06.352905Z",
          "shell.execute_reply": "2021-02-25T10:51:06.353442Z"
        },
        "papermill": {
          "duration": 0.018782,
          "end_time": "2021-02-25T10:51:06.353736",
          "exception": false,
          "start_time": "2021-02-25T10:51:06.334954",
          "status": "completed"
        },
        "tags": [],
        "id": "X0FVjkaiZP5V"
      },
      "source": [
        "# Download Data\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "import requests\n",
        "movies = {}\n",
        "\n",
        "\n",
        "def downaload_dataset():\n",
        "    #downloading data from API:\n",
        "\n",
        "    api_key = \"24370f953f5f3f70890d54a4705b8ff4\"\n",
        "\n",
        "    with open('movies.csv', 'rU') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=';')\n",
        "        for row in reader:\n",
        "            resp = requests.get(\"https://api.themoviedb.org/3/movie/\"+ row[1] +\"?api_key=\"+api_key)\n",
        "            json_resp = resp.json()\n",
        "            # Here I only use the first genre of each movie mentioned\n",
        "            movies[row[0]] = [\n",
        "                              str(row[0]),\n",
        "                              str(json_resp[\"budget\"]),\n",
        "                              str(json_resp[\"genres\"][0][\"id\"]),\n",
        "                              str(json_resp[\"runtime\"]),\n",
        "                              str(json_resp[\"release_date\"].split('-')[0]),\n",
        "                             ]\n",
        "\n",
        "    with open(\"movies_1.csv\", 'wb') as myfile:\n",
        "        wr = csv.writer(myfile)\n",
        "        for key, value in movies.iteritems():\n",
        "            wr.writerow(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:51:06.371309Z",
          "iopub.status.busy": "2021-02-25T10:51:06.370687Z",
          "iopub.status.idle": "2021-02-25T10:51:07.224510Z",
          "shell.execute_reply": "2021-02-25T10:51:07.223788Z"
        },
        "papermill": {
          "duration": 0.863438,
          "end_time": "2021-02-25T10:51:07.224659",
          "exception": false,
          "start_time": "2021-02-25T10:51:06.361221",
          "status": "completed"
        },
        "tags": [],
        "id": "UM84FA-wZP5X"
      },
      "source": [
        "# Pearsonâ€™s Correlation\n",
        "\n",
        "from numpy.random import randn\n",
        "from numpy.random import seed\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def pearson_correl():\n",
        "    data1 = [item[0] for item in DataSet]\n",
        "    data8 = [item[7] for item in DataSet]\n",
        "    # calculate Pearson's correlation\n",
        "\n",
        "\n",
        "    corr, _ = pearsonr(data1, data8)\n",
        "    print('Pearsons correlation: %.3f' % corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:51:07.264344Z",
          "iopub.status.busy": "2021-02-25T10:51:07.258704Z",
          "iopub.status.idle": "2021-02-25T10:51:07.281610Z",
          "shell.execute_reply": "2021-02-25T10:51:07.281074Z"
        },
        "papermill": {
          "duration": 0.049652,
          "end_time": "2021-02-25T10:51:07.281756",
          "exception": false,
          "start_time": "2021-02-25T10:51:07.232104",
          "status": "completed"
        },
        "tags": [],
        "id": "M9PwiRofZP5Y"
      },
      "source": [
        "# nearest neighbour reccomendation\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NearesteighbourEngine:\n",
        "    \n",
        "    DataSet = {}\n",
        "    movies = {}\n",
        "    validation_sets = []\n",
        "    training_sets = []\n",
        "    l1 = []\n",
        "\n",
        "\n",
        "    # Data Normalization:\n",
        "    def normalizeData():\n",
        "        with open(\"movies_1.csv\", 'rb') as myfile:\n",
        "            reader = csv.reader(myfile)\n",
        "            for row in reader:\n",
        "                x = [float(y) for y in row[1:]]\n",
        "                movies[row[0]] = x\n",
        "\n",
        "        dct = {}\n",
        "        for i in range(4):\n",
        "            dct[i] = []\n",
        "\n",
        "        for k, v in movies.iteritems():\n",
        "            for j in range(4):\n",
        "                dct[j].append(v[j])\n",
        "\n",
        "        for key in dct:\n",
        "            high = max(dct[key])\n",
        "            low = min(dct[key])\n",
        "            dct[key] = []\n",
        "            dct[key].append(low)\n",
        "            dct[key].append(high)\n",
        "\n",
        "        for k, v in movies.iteritems():\n",
        "            for i in range(len(v)):\n",
        "                i = int(i)\n",
        "                y = (2 * (v[i] - dct[i][0]) / (dct[i][1] - dct[i][0])) - 1\n",
        "                movies[k][i] = y\n",
        "\n",
        "        # download data from different csv files and mix them together in dataset\n",
        "        with open('train.csv', 'rb') as f:\n",
        "            reader = csv.reader(f)\n",
        "            l1 = list(reader)\n",
        "\n",
        "\n",
        "    for l in l1:\n",
        "        N = [float(x) for x in l[0].split(';')]\n",
        "        features = movies[str(int(N[2]))]\n",
        "        for j in features:\n",
        "            N.append(float(j))\n",
        "\n",
        "        if str(int(N[1])) not in DataSet.keys():\n",
        "            DataSet[str(int(N[1]))] = [N[2:]]\n",
        "        else:\n",
        "            DataSet[str(int(N[1]))].append(N[2:])\n",
        "\n",
        "    #looping through k from 1 to 10 to find proper neighbourhood:\n",
        "    def euclideanDistance(inst1, inst2):\n",
        "        distance = 0\n",
        "        for x in range(2,6):\n",
        "            distance += pow((inst1[x] - inst2[x]), 2)\n",
        "        return math.sqrt(distance)\n",
        "\n",
        "    def getNeighbors(trainingSet, testInstance, k):\n",
        "        distances = []\n",
        "        length = len(testInstance)-1\n",
        "        for x in range(len(trainingSet)):\n",
        "            dist = euclideanDistance(testInstance, trainingSet[x])\n",
        "            distances.append((trainingSet[x], dist))\n",
        "        distances.sort(key=operator.itemgetter(1))\n",
        "        neighbors = []\n",
        "        # find K closest neighbors\n",
        "        for x in range(k):\n",
        "            neighbors.append(distances[x][0])\n",
        "        return neighbors\n",
        "\n",
        "    # get averge of rating of neighbours\n",
        "    def getResponse(neighbors):\n",
        "        sum = 0\n",
        "        for x in range(len(neighbors)):\n",
        "            sum += neighbors[x][1]\n",
        "        return round(sum / len(neighbors))\n",
        "\n",
        "    def getErrorRate(testSet, predictions):\n",
        "        sum = 0\n",
        "        for x in range(len(testSet)):\n",
        "            sum = abs(float(testSet[x][1]) - predictions[x])\n",
        "        return (sum/float(len(testSet)))\n",
        "\n",
        "    errors_hyper = []\n",
        "    \n",
        "    def k_selection():\n",
        "        for k in range(1, 10):\n",
        "            error_set = []\n",
        "            for key in DataSet.keys():\n",
        "                predictions=[]\n",
        "                person1 = DataSet[key]\n",
        "                trainingSet = []\n",
        "                testSet = []\n",
        "                for i in range(int(len(person1)/10)):\n",
        "                    x = person1[random.randint(0, len(person1)-1)]\n",
        "                    testSet.append(x)\n",
        "                    person1.remove(x)\n",
        "                trainingSet = person1\n",
        "\n",
        "                for x in range(len(testSet)):\n",
        "                    neighbors = getNeighbors(trainingSet, testSet[x], k)\n",
        "                    result = getResponse(neighbors)\n",
        "                    predictions.append(result)\n",
        "\n",
        "                error = getErrorRate(testSet, predictions)\n",
        "                error_set.append(error)\n",
        "                errors_hyper.append((k,sum(error_set)/len(error_set)))\n",
        "\n",
        "        final_k = min(errors_hyper, key = lambda t: t[1])[0]\n",
        "\n",
        "\n",
        "    def final_test():\n",
        "        with open('task.csv', 'rb') as f:\n",
        "            reader = csv.reader(f)\n",
        "            your_list = list(reader)\n",
        "        list_1 = []\n",
        "\n",
        "        for l in your_list:\n",
        "            N = [x for x in l[0].split(';')]\n",
        "            user_data = DataSet[N[1]]  # all train data relating to this user\n",
        "            movie_info = movies[N[2]]  # features related to this movie\n",
        "            movie_info = [1,1]+movie_info\n",
        "            neighbors = getNeighbors(user_data, movie_info, final_k)\n",
        "            result = getResponse(neighbors)\n",
        "            N[3] = int(result)\n",
        "            list_1.append(N)\n",
        "\n",
        "        with open(\"submission.csv\", 'wb') as myfile:\n",
        "            wr = csv.writer(myfile, delimiter=';')\n",
        "            for l in list_1:\n",
        "                wr.writerow(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:51:07.330353Z",
          "iopub.status.busy": "2021-02-25T10:51:07.328075Z",
          "iopub.status.idle": "2021-02-25T10:51:07.331050Z",
          "shell.execute_reply": "2021-02-25T10:51:07.331565Z"
        },
        "papermill": {
          "duration": 0.042516,
          "end_time": "2021-02-25T10:51:07.331744",
          "exception": false,
          "start_time": "2021-02-25T10:51:07.289228",
          "status": "completed"
        },
        "tags": [],
        "id": "77awJ1t4ZP5Z"
      },
      "source": [
        "# tree with prunning\n",
        "\n",
        "training_data = [\n",
        "    ['Green', 3, 'Apple'],\n",
        "    ['Yellow', 3, 'Apple'],\n",
        "    ['Red', 1, 'Grape'],\n",
        "    ['Red', 1, 'Grape'],\n",
        "    ['Yellow', 3, 'Lemon'],\n",
        "]\n",
        "\n",
        "\n",
        "max_Depth = 5;\n",
        "\n",
        "\n",
        "def unique_vals(rows, col):\n",
        "    return set([row[col] for row in rows])\n",
        "\n",
        "def class_counts(rows):\n",
        "    counts = {}\n",
        "    for row in rows:\n",
        "        label = row[-1]\n",
        "        if label not in counts:\n",
        "            counts[label] = 0\n",
        "        counts[label] += 1\n",
        "    return counts\n",
        "\n",
        "def is_numeric(value):\n",
        "    # in both conditions returns true\n",
        "    return isinstance(value, int) or isinstance(value, float)\n",
        "\n",
        "\n",
        "# A Question is used to partition a dataset\n",
        "class Question:\n",
        "    def __init__(self, column, value):\n",
        "        self.column = column\n",
        "        self.value = value\n",
        "\n",
        "    def match(self, example):\n",
        "        # Compare the feature value in an example to the feature value in this question.\n",
        "        val = example[self.column]\n",
        "        if is_numeric(val):\n",
        "            return val >= self.value\n",
        "        else:\n",
        "            return val == self.value\n",
        "\n",
        "    def __repr__(self):\n",
        "        # This is just a helper method to print the question in a readable format.\n",
        "        condition = \"==\"\n",
        "        if is_numeric(self.value):\n",
        "            condition = \">=\"\n",
        "        return \"Is %s %s %s?\" % (header[self.column], condition, str(self.value))\n",
        "\n",
        "\n",
        "def partition(rows, question):\n",
        "    \"\"\"Partitions a dataset.\n",
        "    For each row in the dataset, check if it matches the question. If\n",
        "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
        "    \"\"\"\n",
        "    true_rows, false_rows = []\n",
        "    for row in rows:\n",
        "        if question.match(row):\n",
        "            true_rows.append(row)\n",
        "        else:\n",
        "            false_rows.append(row)\n",
        "    return true_rows, false_rows\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gini(rows):\n",
        "    #Calculate the Gini Impurity for a list of rows.\n",
        "    counts = class_counts(rows) # Counts the number of each type of example in a dataset\n",
        "    impurity = 1\n",
        "\n",
        "    for label in counts:\n",
        "        prob_of_label = counts[label] / float(len(rows))\n",
        "        impurity -= prob_of_label**2\n",
        "    return impurity\n",
        "\n",
        "\n",
        "\n",
        "def info_gain(left, right, current_uncertainty):\n",
        "    \"\"\"Information Gain.\n",
        "    The uncertainty of the starting node, minus the weighted impurity of two child nodes.\n",
        "    \"\"\"\n",
        "    p = float(len(left) / len(left) + len(right))\n",
        "    # whatever probabilty of left is, right is 1- p_left\n",
        "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)\n",
        "\n",
        "\n",
        "def find_best_split(rows):\n",
        "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
        "    and calculating the information gain.\"\"\"\n",
        "\n",
        "    best_gain = 0 # keep track of best information gain\n",
        "    best_question = None #  keep train of the feature / value that produced it\n",
        "    current_uncertainty = gini(rows)\n",
        "    n_features = len(rows[0]) - 1 # number of columns\n",
        "\n",
        "    for col in range(n_features): # for each feature\n",
        "        values = set([row[col] for row in rows]) # unique values in the column\n",
        "\n",
        "        for val in values: # for each value:\n",
        "            question = Question(col, val)\n",
        "\n",
        "            # try splitting the dataset:\n",
        "            true_rows, false_rows = partition(rows, question)\n",
        "            # Skip this split if it doesn't divide the\n",
        "            # dataset.\n",
        "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
        "                continue\n",
        "\n",
        "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
        "\n",
        "            if gain >= best_gain:\n",
        "                best_gain, best_question = gain, question\n",
        "\n",
        "    return best_gain, best_question\n",
        "\n",
        "class leaf:\n",
        "    \"\"\"A Leaf node classifies data.\n",
        "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
        "    it appears in the rows from the training data that reach this leaf.\n",
        "    \"\"\"\n",
        "    def __init__(self, rows):\n",
        "        self.predictions = class_counts(rows)\n",
        "\n",
        "\n",
        "\n",
        "class Decision_Node:\n",
        "    \"\"\"A Decision Node asks a question.\n",
        "    This holds a reference to the question, and to the two child nodes.\n",
        "    \"\"\"\n",
        "    def __init__(self, question, true_branch, false_branch):\n",
        "        self.question = question\n",
        "        self.true_branch = true_branch\n",
        "        self.false_branch = false_branch\n",
        "\n",
        "\n",
        "def build_tree(rows, depth):\n",
        "    gain, question = find_best_split(rows)\n",
        "\n",
        "    # STOP POINT OF RECURSIVE TREE\n",
        "    if gain == 0 or depth >= max_Depth:\n",
        "        return Leaf(rows)\n",
        "\n",
        "    # If we reach here, we have found a useful feature / value to partition on.\n",
        "    true_rows, false_rows = partition(rows, question)\n",
        "\n",
        "    # Recursively build the true branch.\n",
        "    true_branch = build_tree(true_rows, depth+1)\n",
        "\n",
        "    # Recursively build the false branch.\n",
        "    false_branch = build_tree(false_rows, depth+1)\n",
        "\n",
        "    # Return a Question node. This records the best feature / value to ask at this point, as well as the branches to follow dependingo on the answer. (branch with two more nodes to follow it)\n",
        "    return Decision_Node(question, true_branch, false_branch)\n",
        "\n",
        "# we use the calissify function to make classification for each row  of dataset via the decision_tree we made\n",
        "def classify(row, node):\n",
        "    # Base case: we've reached a leaf\n",
        "    if isinstance(node, Leaf):\n",
        "        return node.predictions\n",
        "    # you will repeat this loop until you reach leaf\n",
        "    if node.question.match(row):\n",
        "        return classify(row, node.true_branch)\n",
        "    else:\n",
        "        return classify(row, node.false_branch)\n",
        "\n",
        "\n",
        "def calculate_accuracy(data):\n",
        "    sum = 0\n",
        "    for row in training_data:\n",
        "        if row[-1] == classify(row, data):\n",
        "            sum += 1\n",
        "    return float( sum / len(data) )\n",
        "\n",
        "\n",
        "# treat depth as hyper-parameter for tree pruning:\n",
        "def prune():\n",
        "    error_set = []\n",
        "    for i in range(4, 10):\n",
        "        max_Depth = i\n",
        "        testSet = []\n",
        "        data_temp = training_data.copy()\n",
        "        for i in range(int(len(data_temp)/5)):\n",
        "            x = data_temp[str(random.choice(len(data_temp)))]\n",
        "            testSet.append(x)\n",
        "            data_temp.remove(x)\n",
        "\n",
        "        my_tree = build_tree(training_data, 1)\n",
        "        accur = calculate_accuracy(testSet)\n",
        "        error_set.append([max_Depth, accur])\n",
        "\n",
        "    max_Depth = min(error_set, key = lambda t: t[1])[0]\n",
        "    my_tree = build_tree(training_data, 1)\n",
        "\n",
        "\n",
        "    # Evaluate\n",
        "    testing_data = [\n",
        "        ['Green', 3, 'Apple'],\n",
        "        ['Yellow', 4, 'Apple'],\n",
        "        ['Red', 2, 'Grape'],\n",
        "        ['Red', 1, 'Grape'],\n",
        "        ['Yellow', 3, 'Lemon'],\n",
        "    ]\n",
        "\n",
        "    for row in testing_data:\n",
        "        classify(row, my_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:51:07.378579Z",
          "iopub.status.busy": "2021-02-25T10:51:07.376473Z",
          "iopub.status.idle": "2021-02-25T10:51:07.379260Z",
          "shell.execute_reply": "2021-02-25T10:51:07.379772Z"
        },
        "papermill": {
          "duration": 0.040706,
          "end_time": "2021-02-25T10:51:07.379964",
          "exception": false,
          "start_time": "2021-02-25T10:51:07.339258",
          "status": "completed"
        },
        "tags": [],
        "id": "0rSlAdYEZP5d"
      },
      "source": [
        "# item_item_collab_filter\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ItemItemCollabFilter:\n",
        "\n",
        "    DataSet = {} # for users\n",
        "    DataSet2 = {} # for movies\n",
        "    relations = []\n",
        "    relations = {}\n",
        "\n",
        "    def getData():\n",
        "        with open('train.csv', 'rb') as f:\n",
        "            reader = csv.reader(f)\n",
        "            l1 = list(reader)\n",
        "\n",
        "        for l in l1:\n",
        "            N = [int(x) for x in l[0].split(';')]\n",
        "\n",
        "            if str(int(N[1])) not in DataSet.keys():\n",
        "                DataSet[str(int(N[1]))] = [N[1:]]\n",
        "            else:\n",
        "                DataSet[str(int(N[1]))].append(N[1:])\n",
        "\n",
        "            if str(int(N[2])) not in DataSet2:\n",
        "                DataSet2[str(int(N[2]))] = [[N[1], N[3]]]\n",
        "            else:\n",
        "                DataSet2[str(int(N[2]))].append([N[1], N[3]])\n",
        "\n",
        "\n",
        "    def movieRelation():\n",
        "        for k, v in DataSet2.iteritems():\n",
        "            # averge rating of the movie among all users\n",
        "            movie_avg =  sum(x[1] for x in v) / float(len(v))\n",
        "            relations[k] = []\n",
        "            for key, value in DataSet2.iteritems():\n",
        "                up = 0\n",
        "                temp1 = []\n",
        "                temp2 = []\n",
        "\n",
        "                if key != k:\n",
        "                    other_avg =  sum(x[1] for x in value)/ float(len(value))\n",
        "                    for item1 in v:\n",
        "                        for item2 in value:\n",
        "                            if (item1[0] == item2[0]) and (item1[1] !=0 or item2[1] != 0):\n",
        "                                up += (item1[1]-movie_avg) * (item2[1] - other_avg)\n",
        "\n",
        "                                temp1.append(item1[1])\n",
        "                                temp2.append(item2[1])\n",
        "                                break\n",
        "\n",
        "                    sum1 = 0\n",
        "                    sum2 = 0\n",
        "                    for num in temp1:\n",
        "                        sum1+=(num - movie_avg)**2\n",
        "                    for num in temp2:\n",
        "                        sum2+= (num - other_avg)**2\n",
        "                    down = (sum1**0.5)*(sum2**0.5)\n",
        "                    rel = float(up) / down\n",
        "                    relations[k].append((key, rel))\n",
        "\n",
        "    #movieRelation()\n",
        "\n",
        "\n",
        "    def findSimilar(userID, k, movieID):\n",
        "        user = DataSet[str(userID)]\n",
        "\n",
        "        # if user has same rating for almost all movies then he will probably have same one for this movie:\n",
        "        variance = np.var([x[2] for x in user])\n",
        "        if variance <= 0.6:\n",
        "            return sum([x[2] for x in user]) / float(len(user))\n",
        "\n",
        "        # find k closet movies from this user's ratings\n",
        "        similar = relations[str(movieID)]\n",
        "        similar = sorted(similar, key=lambda x : x[1], reverse=True)\n",
        "        similar = similar[:k]\n",
        "        down1 = 0.01\n",
        "        up1 = 0\n",
        "        for tuple in similar:\n",
        "            l = DataSet[str(userID)]\n",
        "            for x in l:\n",
        "                if int(x[1]) == int(tuple[0]):\n",
        "                    down1 += abs(tuple[1])\n",
        "                    up1 += tuple[1] * x[2]\n",
        "                    break\n",
        "        return round(float(up1)/ down1)\n",
        "\n",
        "        error_set = []\n",
        "        for k in range(2, 15):\n",
        "            testSet = []\n",
        "            data_temp = DataSet.copy()\n",
        "\n",
        "            for i in range(int(len(data_temp)/10)):\n",
        "                x = data_temp[str(random.choice(data_temp.keys()))]\n",
        "                testSet.append(x)\n",
        "                del data_temp[str(x[0][0])]\n",
        "\n",
        "            user_error = []\n",
        "            for user in testSet:\n",
        "                temp = [] # errors of all movies of a single user\n",
        "                for rating in user:\n",
        "                    rate_guess = findSimilar(user[0][0], k, rating[1])\n",
        "                    temp.append(abs(rating[2] - rate_guess))\n",
        "                user_error.append(sum(temp)/len(user))\n",
        "            error_set.append([k, sum(user_error)/len(user_error)])\n",
        "\n",
        "        final_k = min(error_set, key = lambda t: t[1])[0]\n",
        "\n",
        "    def submit_results():\n",
        "        with open('task.csv', 'rb') as g:\n",
        "            reader1 = csv.reader(g)\n",
        "            your_list = list(reader1)\n",
        "\n",
        "        list_1 = []\n",
        "        for l in your_list:\n",
        "            N = [x for x in l[0].split(';')]\n",
        "            user = DataSet[str(N[1])]\n",
        "\n",
        "            rate_guess = findSimilar(user[0][0], final_k, N[2])\n",
        "            N[3] = int(rate_guess)\n",
        "            list_1.append(N)\n",
        "\n",
        "        with open(\"submission.csv\", 'wb') as myfile:\n",
        "            wr = csv.writer(myfile, delimiter=';')\n",
        "            for l in list_1:\n",
        "                wr.writerow(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:51:07.415750Z",
          "iopub.status.busy": "2021-02-25T10:51:07.410570Z",
          "iopub.status.idle": "2021-02-25T10:51:07.432545Z",
          "shell.execute_reply": "2021-02-25T10:51:07.431927Z"
        },
        "papermill": {
          "duration": 0.044959,
          "end_time": "2021-02-25T10:51:07.432702",
          "exception": false,
          "start_time": "2021-02-25T10:51:07.387743",
          "status": "completed"
        },
        "tags": [],
        "id": "CihT5ZxyZP5f"
      },
      "source": [
        "import csv\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "import numpy as np\n",
        "\n",
        "class UserUserCollabFilter:\n",
        "\n",
        "    DataSet = {}\n",
        "    listSet = []\n",
        "    \n",
        "    def readData():\n",
        "        with open('train.csv', 'rb') as f:\n",
        "            reader = csv.reader(f)\n",
        "            l1 = list(reader)\n",
        "\n",
        "\n",
        "        for l in l1:\n",
        "            N = [int(x) for x in l[0].split(';')]\n",
        "\n",
        "            if str(int(N[1])) not in DataSet.keys():\n",
        "                DataSet[str(int(N[1]))] = [N[1:]]\n",
        "            else:\n",
        "                DataSet[str(int(N[1]))].append(N[1:])\n",
        "            listSet.append(N[1:])\n",
        "            \n",
        "\n",
        "    # [0 :'USERID', 1 :'MOVIEID', 2 :'MOVIE_RATING']\n",
        "    # find similar users that have already rated this movie:\n",
        "    def findSimilar(userID, Data, k, movieID):\n",
        "        user = DataSet[str(userID)]\n",
        "\n",
        "        # if user has same rating for almost all movies then he will probably have same one for this movie:\n",
        "        variance = np.var([x[2] for x in user])\n",
        "        if variance <= 0.6:\n",
        "            return sum([x[2] for x in user]) / float(len(user))\n",
        "\n",
        "        similar = []\n",
        "        user_avg = sum([x[2] for x in user]) / float(len(user))\n",
        "        for key, v in DataSet.iteritems():\n",
        "            containsMovie = any(int(movieID) == int(x[1]) for x in v)\n",
        "            variance1 = np.var([x[2] for x in v])\n",
        "            if (key != userID) and containsMovie and (variance1 > 1):\n",
        "                    avg = sum([x[2] for x in v]) / float(len(v))\n",
        "                    up = 0\n",
        "                    temp1 = []\n",
        "                    temp2 = []\n",
        "                    for item in user:\n",
        "                        for item2 in v:\n",
        "                            if item[1] == item2[1]:\n",
        "                                up += (item2[2] - avg)*(item[2] - user_avg)\n",
        "\n",
        "                                temp1.append(item[2])\n",
        "                                temp2.append(item2[2])\n",
        "                                break\n",
        "                    sum1 = 0\n",
        "                    sum2 = 0\n",
        "\n",
        "                    # for current user\n",
        "                    for num in temp1:\n",
        "                        sum1 += (num - user_avg)**2\n",
        "\n",
        "                    # for all neighbours\n",
        "                    for num in temp2:\n",
        "                        sum2 += (num - avg)**2\n",
        "\n",
        "                    down = (sum1**0.5)*(sum2**0.5)\n",
        "\n",
        "                    similarity = up / down\n",
        "                    similar.append((key,similarity))\n",
        "\n",
        "        similar = sorted(similar, key=lambda x: x[1], reverse=True)\n",
        "        similar = similar[:k]\n",
        "        # rate movie based on what other users said about that movie\n",
        "        l1 = []\n",
        "        for tuple in similar:\n",
        "            neighbour_data = DataSet[str(tuple[0])]\n",
        "            for x in neighbour_data:\n",
        "                if int(x[1]) == int(movieID):\n",
        "                    l1.append([x[2], tuple[1]])\n",
        "                    break\n",
        "\n",
        "        down1 = sum([x[1] for x in l1])\n",
        "        up1 = sum([x[0]*x[1] for x in l1])\n",
        "        return round(float(up1)/down1)\n",
        "\n",
        "\n",
        "        def guess_results():\n",
        "            error_set = []\n",
        "            for k in range(2, 10):\n",
        "                testSet = []\n",
        "                #list = DataSet.keys()\n",
        "                data_temp = DataSet.copy()\n",
        "\n",
        "                for i in range(int(len(data_temp)/20)):\n",
        "                    x = data_temp[str(random.choice(data_temp.keys()))]\n",
        "                    testSet.append(x)\n",
        "                    del data_temp[str(x[0][0])]\n",
        "\n",
        "                user_error = []\n",
        "                for user in testSet:\n",
        "                    temp = [] # errors of all movies of a single user\n",
        "                    for rating in user:\n",
        "                        rate_guess = findSimilar(user[0][0], data_temp, k, rating[1])\n",
        "                        temp.append(abs(rating[2] - rate_guess))\n",
        "                    user_error.append(sum(temp)/len(user))\n",
        "\n",
        "                error_set.append([k, sum(user_error)/len(user_error)])\n",
        "\n",
        "            final_k = min(error_set, key = lambda t: t[1])[0]\n",
        "\n",
        "            with open('task.csv', 'rb') as g:\n",
        "                reader1 = csv.reader(g)\n",
        "                your_list = list(reader1)\n",
        "\n",
        "            list_1 = []\n",
        "            for l in your_list:\n",
        "                N = [x for x in l[0].split(';')]\n",
        "                user = DataSet[str(N[1])]\n",
        "                rate_guess = findSimilar(user[0][0], DataSet, final_k, N[2])\n",
        "                N[3] = int(rate_guess)\n",
        "                list_1.append(N)\n",
        "\n",
        "            with open(\"submission.csv\", 'wb') as myfile:\n",
        "                wr = csv.writer(myfile, delimiter=';')\n",
        "                for l in list_1:\n",
        "                    wr.writerow(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-25T10:51:07.467859Z",
          "iopub.status.busy": "2021-02-25T10:51:07.467052Z",
          "iopub.status.idle": "2021-02-25T10:51:07.469792Z",
          "shell.execute_reply": "2021-02-25T10:51:07.469169Z"
        },
        "papermill": {
          "duration": 0.029233,
          "end_time": "2021-02-25T10:51:07.469928",
          "exception": false,
          "start_time": "2021-02-25T10:51:07.440695",
          "status": "completed"
        },
        "tags": [],
        "id": "MFWxG4wFZP5f"
      },
      "source": [
        "# K Nearest Neighbour\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "\n",
        "def loadDataset(filename, split, trainingSet=[] , testSet=[]):\n",
        "    # here add normalization to input features and set them to 0-1 range\n",
        "\twith open(filename, 'rb') as csvfile:\n",
        "\t    lines = csv.reader(csvfile)\n",
        "\t    dataset = list(lines)\n",
        "\t    for x in range(len(dataset)-1):\n",
        "\t        for y in range(4):\n",
        "\t            dataset[x][y] = float(dataset[x][y])\n",
        "\t        if random.random() < split:\n",
        "\t            trainingSet.append(dataset[x])\n",
        "\t        else:\n",
        "\t            testSet.append(dataset[x])\n",
        "\n",
        "\n",
        "def euclideanDistance(instance1, instance2, length):\n",
        "\tdistance = 0\n",
        "\tfor x in range(length):\n",
        "\t\tdistance += pow((instance1[x] - instance2[x]), 2)\n",
        "\treturn math.sqrt(distance)\n",
        "\n",
        "def getNeighbors(trainingSet, testInstance, k):\n",
        "\tdistances = []\n",
        "\tlength = len(testInstance)-1\n",
        "\tfor x in range(len(trainingSet)):\n",
        "\t\tdist = euclideanDistance(testInstance, trainingSet[x], length)\n",
        "\t\tdistances.append((trainingSet[x], dist))\n",
        "\tdistances.sort(key=operator.itemgetter(1))\n",
        "\tneighbors = []\n",
        "\tfor x in range(k):\n",
        "\t\tneighbors.append(distances[x][0])\n",
        "\treturn neighbors\n",
        "\n",
        "def getResponse(neighbors):\n",
        "\tclassVotes = {}\n",
        "\tfor x in range(len(neighbors)):\n",
        "\t\tresponse = neighbors[x][-1]\n",
        "\t\tif response in classVotes:\n",
        "\t\t\tclassVotes[response] += 1\n",
        "\t\telse:\n",
        "\t\t\tclassVotes[response] = 1\n",
        "\tsortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
        "\treturn sortedVotes[0][0]\n",
        "\n",
        "def getAccuracy(testSet, predictions):\n",
        "\tcorrect = 0\n",
        "\tfor x in range(len(testSet)):\n",
        "\t\tif testSet[x][-1] == predictions[x]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn (correct/float(len(testSet))) * 100.0\n",
        "\n",
        "def main():\n",
        "\t# prepare data\n",
        "\ttrainingSet=[]\n",
        "\ttestSet=[]\n",
        "\tsplit = 0.67\n",
        "\tloadDataset('iris.data', split, trainingSet, testSet)\n",
        "\tprint('Train set: ' + repr(len(trainingSet)))\n",
        "\tprint('Test set: ' + repr(len(testSet)))\n",
        "\t# generate predictions\n",
        "\tpredictions=[]\n",
        "\tk = 3\n",
        "\tfor x in range(len(testSet)):\n",
        "\t\tneighbors = getNeighbors(trainingSet, testSet[x], k)\n",
        "\t\tresult = getResponse(neighbors)\n",
        "\t\tpredictions.append(result)\n",
        "\t\tprint('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))\n",
        "\taccuracy = getAccuracy(testSet, predictions)\n",
        "\tprint('Accuracy: ' + repr(accuracy) + '%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}